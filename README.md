# Capstone Project: Ghiblify! Cartoonizing Real-Life Images using GANs

In this project, we attempt to use GANs to cartoonize Real-Life Images.

## Problem Statement

Animes are hand-drawn and computer-generated animation originating from Japan. Manual creation of animes is a very laborious process which requires artistic skills. Anime artists need to hand-draw every line and fill in the colors of scenes in a full feature anime film. This project aims to answer the following questions:

1. Explore how Generative Adversarial Networks (GANs) can help to relieve the heavy workload of anime artists through cartoonizing real-life images and serve as a fun application for social network apps.

2. Attempt to optimize the parameters and architecture of GANs to achieve better results in real-life image to cartoon translation.

## Introduction and History of GANs
GANs is a type of generative machine learning framework using deep learning methods which was  invented by Ian Goodfellow and his team in 2014. It is an unsupervised learning model that generates new samples through automically learning of patterns in input data. 

An often quoted analogy of GANs is that of a fake wine maker who attempts to sell his fake wine to a wine shop. Over time, the fake wine maker gets better at producing wine after learning from his failures to con the wine shop into accepting his wine. At the same time, the wine shop continually learns to better distinguish between authentic and fake wine through feedback from the customer who bought the wines from the shop. Using this analogy, we can illustrate the main components of a basic GANs([*source*](https://towardsdatascience.com/demystifying-generative-adversarial-networks-c076d8db8f44)):
![1](https://user-images.githubusercontent.com/87510577/140941924-337529be-2dd2-4ce0-8e46-76dda8fec474.jpeg)

**Building Blocks of GANs**

At its most basic form, a GAN has the following components:

1. Noise vector: The Generator takes in a noise vector and outputs a fake image
2. Real Dataset: Real images of a particular domain which is fed into the discriminator.
3. Generator: Usually a convolutional neural network (CNN) as GANs are usually used for image generation. Plays the role of generating fake images of a particular domain
4. Fake Data: Fake images of a particular domain generated by the Generator in the hope passing it off as real to the Discriminator
4. Discriminator: Also usually a CNN which classifies an image as being real or fake with respect to a particular domain. It takes in both the fake images generated by the Generator and the Real images.

## Images Dataset Sources 

The objective of the project is to cartoonize real-life images such that they appear to look like screen-shots of anime.  Therefore, we will require images from these two domains: 
1. Real-Life Images
2. Anime Images

The following documents the sources of the images:

**Real-life Images**
1. Unsplash (6387 images) ([*source*](https://unsplash.com/))
2. US Roadside Attraction (1976 images) ([*source*](https://www.kaggle.com/headsortails/us-roadside-attractions-photographs))
3. Female Portrait (3318 images) ([*source*](https://www.kaggle.com/yewtsing/pretty-face))
4. Female Portrait for Testing (11 images) ([*source*](https://generated.photos/))

**Anime Images**
1. Ghibli Animes (6079 images) ([*source*](https://www.netflix.com/))
2. Anime Female Portrait (3400 images) ([*source*](https://www.kaggle.com/arnaud58/selfie2anime))

All images have been cropped to 1:1 aspect ratio and are either 256x256 or 512x512 in pixel size

## Analysis Methodology

1. Images from both the Anime and Real-Life domains were curated as necessary to ensure uniformity in themes (e.g. landscape, portrait etc). 

2. The images are then pre-processed to 1:1 aspect ratio and 256x256 or 512x512 pixels in size before inputting into the GAN model

3. GANs model were trained using various combination of themed Anime and Real-Life images, hyperparameters and GAN architecture

4. Since GAN is an unsupervised learning model, there is no objective metrics such as accuracy and loss. The best model is therefore chosen based on the qualitative review of generated images i.e. how well cartoonized the real-life images are.

## Models Summary
### 1. Basic GAN with Noise Vectors Input
The Basic GAN with noise vectors input is capable of generating new anime based on feeding anime images to the Discriminator, but it depends on the cleanliness of the anime image dataset.

<div align="center">Generated Anime at 80 epochs</div>
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140944881-fcbb931b-638e-4650-9885-a3d52de91c90.png">
</p>

### 2. Basic GAN with Real-Life Image Input
We then experiment with replacing the noise vector input with a real-life images in the Basic GAN. It did not work because the Basic GAN lacks a loss function for retaining content of the input image

<div align="center">Generated Anime at 80 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140945899-f6e0b63a-c53c-4ea7-91a3-fd7f87525f70.png">
</p>

### 3. CycleGAN with Mixed Theme Anime Images
CycleGAN comes with the cyclic consistency loss function required to retain input image content. The anime style is successfully transferred onto most of the real life image, with the content of the real-life image retained on the generated anime image. However, we see some anime-like pattern on areas of some generated images which is supposed to be the sky on the original real-life image on both the training and test images. The Ghibli Anime images contains a mixture of landscape, human face, and monsters. Therefore, the CycleGAN model would have transferred some of the facial features in the Anime dataset onto the real-life image. The takeaway here is to use a image dataset with uniform theme to train the CycleGAN model.

<div align="center">Generated Anime at 80 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140946627-75e5db19-a361-4eda-9e92-f679b89ba2bd.png">
</p>

<div align="center">Generated Anime using test image at 200 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140946639-d6304c93-5713-43bb-be09-fe18215d683f.png">
</p>

### 4. CycleGAN with Curated Anime Images
The curated Ghibli dataset seems to have improved the generated anime images. Generally, there are less incoherent details in the generated images. On some real-life images with clear blue skies, the CycleGAN model even generated anime images that fill the skies with clouds. A key learning point is that although there is no need for paired images for CycleGAN, it is important that the themes of the input and target domain matches for better performance.

<div align="center">Generated Anime at 80 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140946841-be6a9397-72c0-4dfd-aacc-7b2a82b831eb.png">
</p>

<div align="center">Generated Anime using test image at 200 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140946855-e3dd857b-8165-4ee5-abe5-c55ed27971fe.png">
</p>

### 5. CycleGAN with Female Portrait
The CycleGAN model did well with hair but got progressively worse with facial features. It was unable to learn the correct positions for the eyes quite early on during the training. A possible solution would be to increase the value of LAMBDA so that cyclic consistency loss and identity loss is given higher importance with respect to adversarial loss. This would theoretically guide the model to prioritize the position of the facial features and reduce distortion.

<div align="center">Generated Anime using test image at 60 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947176-74724183-9018-4e73-9cf7-1f95b6a827cc.png">
</p>

<div align="center">Generated Anime using test image at 60 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947202-cfffd363-fa87-42c7-aba2-ad648162ea20.png">
</p>


### 6. CycleGAN LAMBDA = 20 with Female Portrait
The model had problem with facial features right from the start despite the increase in LAMBDA. The problem with image translation may have its origin in the architecture of the generator and discriminator instead of the optimization of LAMBDA.

<div align="center">Generated Anime using test image at 40 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947363-3d66df7a-a5c9-40e2-8866-d217512f6d0a.png">
</p>

<div align="center">Generated Anime using test image at 40 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947378-531eab14-1f11-46ea-8628-0c111f8299be.png">
</p>

### 7. CycleGAN ResNet with Female Portrait
The GAN model with ResNet in the generator shows improvement in generating anime images where the facial features such as eyes and mouth are in correct position. However the quality of anime portrait generated is still lacking.

<div align="center">Generated Anime using test image at 40 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947591-17fc4c4d-3d97-4fbe-bb3b-92a2ea5b4f6b.png">
</p>

<div align="center">Generated Anime using test image at 40 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947614-6991ef85-3f2a-471f-b781-5660116c1101.png">
</p>

### 8. CycleGAN ResNet ReflectionPadding2D with Female Portrait
The inclusion of the ReflectionPadding2D layer improved the image generated tremendously. Not only were the facial features in the right position, the content of the orginal portrait image was clearly visible in the generated photo in terms of hair style, face shape and emotion

<div align="center">Generated Anime using test image at 100 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947882-44d0e8a6-3b15-4825-ad0b-92e9b9d5a7e5.png">
</p>

<div align="center">Generated Anime using test image at 100 epochs</div>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140947874-b196bc01-5fe6-47a0-a772-a65bcaf6e434.png">
</p>

Examples generated at other epochs during training:
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140948111-6631dac8-a57f-4e00-a047-620460ff61cd.png">
</p>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140948119-9972e2f3-2ebb-417b-b447-48b634e6efa8.png">
</p>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140948121-63280deb-cdb0-45d7-8315-562ffec2be14.png">
</p>
<p align="center">
<p align="center">
  <img src="https://user-images.githubusercontent.com/87510577/140948124-7194c2ab-59ac-40f2-a224-d708717c1a94.png">
</p>

## Key Takeaways

### Key Conclusions and Recommendations

The result of this project proves beyond doubt that GANs model could be a valuable tool in the arsenal of the anime artists if they wish to recreate real-life scenes for anime movies. The cartoonization of selfies into anime characters would also be a fun application for social network apps.

The following are some tips on how to optimize GANs for the best cartoonization result:

1. Cyclic consistency loss in CycleGAN helps to retain content from the the input real-life image without the use of paired images data
2. Landscape and scenery images can be cartoonized using a CycleGAN with a U-Net Generator. However, both the real-life and anime images used for training needs to be clean and of the same theme.
3. Cartoonization of portraits is sensitive to the position of facial features. ResNet performs better than U-Net in maintaining facial features position. A possible reason could be that ResNet has about one-third as much parameters as U-Net (11 million vs 30 million) and has fewer downsampling which is better for color and style transfer. A higher number of parameters would make them hard for the model to learn without paired data.
4. ReflectionPadding2D improves the quality of the generated anime portrait tremendously. Using ReflectionPadding2D instead of Zero Padding as a layer in front of the Conv2D layer at the image input may have helped to keep the data distribution of the input image intact and improved the generated anime quality.

## Future Improvements

1. CycleGAN is computationally expensive as it involves running 2 GANs simultaneously. A new architecture can be explored where only 1 GAN is required and a triplet loss function is used to retain content of the input image.
2. Explore cartoonizing a video instead of an image


